{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import wandb\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset,DatasetDict\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model_state_dict\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rich import (inspect, print, pretty)\n",
    "from rich.console import Console\n",
    "from rich.syntax import Syntax\n",
    "pretty.install()\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomInvoiceDataset(Dataset):\n",
    "    def __init__(self, json_folder, text_folder):\n",
    "        self.text_files = [os.path.join(text_folder, file) for file in os.listdir(text_folder)]\n",
    "        self.json_files = [os.path.join(json_folder, file) for file in os.listdir(json_folder)]\n",
    "        self.text_files.sort()\n",
    "        self.json_files.sort()\n",
    "\n",
    "        assert len(self.text_files) == len(self.json_files), \"Mismatch in number of files\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        with open(self.text_files[index],'r') as file:\n",
    "            text_content = file.read()\n",
    "            lines  = text_content.splitlines()\n",
    "            list_of_lists = [ast.literal_eval(line) for line in lines]\n",
    "\n",
    "        with open(self.json_files[index], 'r') as file:\n",
    "            json_content = json.load(file)\n",
    "        return list_of_lists, str(json_content)\n",
    "    @property\n",
    "    def features(self):\n",
    "        return ('text','json')\n",
    "    @property\n",
    "    def num_rows(self):\n",
    "        return len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomInvoiceDataset(\"/home/huynv43/langchain_rag/data/json\", \"/home/huynv43/langchain_rag/data/txt_dataset_with_coor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "total_size = len(data)\n",
    "print(total_size)\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_size = int(total_size * train_ratio)\n",
    "valid_size = int(total_size * valid_ratio)\n",
    "test_size = total_size - train_size - valid_size \n",
    "train_dataset, valid_dataset, test_dataset = random_split(data, [train_size, valid_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_instruction(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "You are medical expert, and medical data engineer with many years on working with complex medical receipt structure. \n",
    "I need you parse, detect, recognize and convert following medical receipt OCR image result into structure medical receipt format. \n",
    "the outout mus be a well-formed json object.```json\n",
    "\n",
    "### Input:\n",
    "{sample[0]}\n",
    "\n",
    "### Output:\n",
    "{sample[1]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "print(format_train_instruction(train_dataset[randrange(len(train_dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = format_train_instruction(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_datasets(tokenizer,train_data, test_data, valid_data ,data_dir=None,seq_length=2048,num_workers=6,streaming=False,size_valid_set=10,shuffle_buffer=1000):\n",
    "    train_data = train_data\n",
    "    valid_data = valid_data\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer)\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=format_train_instruction,\n",
    "        infinite=True,\n",
    "        seq_length=seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        formatting_func=format_train_instruction,\n",
    "        infinite=False,\n",
    "        seq_length=seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "hf_token = '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'\n",
    "\n",
    "use_flash_attention = False\n",
    "# Hugging Face model id\n",
    "#model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated \"meta-llama/Llama-2-7b-hf\n",
    "#model_id=\"PY007/TinyLlama-1.1B-intermediate-step-240k-503b\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "##quantization_config=bnb_config, \n",
    "# Load model and tokenizer\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             #load_in_8bit=True,      \n",
    "                                             quantization_config=bnb_config,  \n",
    "                                             token = hf_token,\n",
    "                                             trust_remote_code=True,                                                  \n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frezee the model\n",
    "for param in model_8bit.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model_8bit.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model_8bit.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model_8bit.lm_head = CastOutputToFloat(model_8bit.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8bit.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_8bit.get_memory_footprint()/1024/1024/1024, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_8bit.config.max_position_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_8bit.hf_device_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # skip this time\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "## prepare model for training\n",
    "model = prepare_model_for_kbit_training(model_8bit)\n",
    "base_model = get_peft_model(model, peft_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"Medical-kie\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"./results/mistral7b_ocr_to_json_5_without_torch\"\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "BATCH_SIZE=128\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE=3\n",
    "PER_DEVICE_EVAL_BATCH_SIZE=1\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // PER_DEVICE_TRAIN_BATCH_SIZE\n",
    "SAVE_STEPS=20\n",
    "LOGGING_STEPS=10\n",
    "LEARNING_RATE=2e-4 #3e-4\n",
    "TRAIN_STEPS=150  #300\n",
    "#WARM_UP_STEPS=50  or ratio \n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,        \n",
    "    gradient_accumulation_steps=2, ## GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,        \n",
    "    optim=\"paged_adamw_32bit\",  \n",
    "    logging_steps=LOGGING_STEPS,    \n",
    "    save_total_limit=2,  \n",
    "    save_strategy=\"epoch\",    \n",
    "    learning_rate=2e-4,            ## LEARNING_RATE,    \n",
    "    fp16=True,\n",
    "    # tf32=True,        \n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,             ## warmup_steps=WARM_UP_STEPS,    \n",
    "    lr_scheduler_type=\"constant\",  ##\"cosine\"   \n",
    "    disable_tqdm=True,              # disable tqdm since with packing values are in correct    \n",
    "    #max_steps=TRAIN_STEPS,\n",
    "    report_to=\"wandb\",\n",
    "    #save_steps=SAVE_STEPS,\n",
    "    #group_by_length=False,\n",
    "    #remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",  #steps\n",
    "    run_name=\"sft_mistral7b_colorist\",\n",
    "\n",
    ")\n",
    "train_dataset, eval_dataset = create_datasets(base_tokenizer, train_dataset, test_dataset, valid_dataset, seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer,DataCollatorForCompletionOnlyLM\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,  ## make sure group_by_length=False\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=base_tokenizer,\n",
    "    args=training_args,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.pretraining_tp = 1\n",
    "base_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "## pytorch optimization \n",
    "old_state_dict = base_model.state_dict\n",
    "base_model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(base_model, type(base_model)) \n",
    "\n",
    "# Enable cuDNN auto-tuner - NVIDIA cuDNN supports many algorithms to compute a convolution. \n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "# save model\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "base_tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(\"nguyenhuy/mistral7b_ocr_to_json-lora\", token = \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "output_dir=\"/home/huynv43/langchain_rag/sft_mistral/results/mistral7b_ocr_to_json_5_without_torch/final_checkpoint\"\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "peft_model_id = output_dir\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "print(config.base_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                                 # return_dict=True,\n",
    "                                                 # load_in_4bit=True,                                                 \n",
    "                                                 device_map=\"auto\",\n",
    "                                                 trust_remote_code=True,  \n",
    "                                                 low_cpu_mem_usage=True,                                                \n",
    "                                                 torch_dtype=torch.bfloat16\n",
    "                                                )\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"/home/huynv43/langchain_rag/sft_mistral/results/mistral7b_ocr_to_json_5_without_torch/final_checkpoint\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", local_files_only=True, \n",
    "    load_in_4bit=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
