{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "hf_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import  AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = '/home/huynv43/langchain_rag/dbyte'\n",
    "model_embeddings = \"keepitreal/vietnamese-sbert\"\n",
    "cache_dir = '/home/huynv43/langchain_rag/tmp'\n",
    "model_name_or_path = \"bkai-foundation-models/vietnamese-llama2-7b-120GB\"\n",
    "# model_name_or_path = \"vilm/vinallama-7b\"\n",
    "# model_name_or_path = \"LR-AI-Labs/vbd-llama2-7B-50b-chat\"\n",
    "# model_name_or_path = 'mistralai/Mistral-7B-v0.1'\n",
    "model_kwargs = {'device': 'cuda:0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             trust_remote_code=True,\n",
    "                                             token = hf_token,\n",
    "                                             quantization_config=bnb_config\n",
    "                                             )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, model_kwargs=model_kwargs, token = hf_token, padding_side=\"left\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            do_sample = True,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=120,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "            streamer=streamer,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline = text_pipeline, model_kwargs={\"temperature\": 0.1, \"max_length\":512,'device': 'cuda:0',\"stop\":\"\\n\"})\n",
    "embeddings = HuggingFaceEmbeddings(model_name = model_embeddings,  model_kwargs = model_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in question and answering. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "                        Remember:\n",
    "                        - always answer in Vietnamese.\n",
    "                        - don't try to generate other answers and questions.\n",
    "                        - to be honest if you don't know, don't try to answer.\n",
    " \n",
    "                        knowledge : {context}\n",
    " \n",
    " \n",
    "                        question : {question}\n",
    " \n",
    "                        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in medical field. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "Remember:\n",
    "- don't add \"Answer\" before answer\n",
    "- always answer in Vietnamese.\n",
    "- don't try to generate other answers and questions.\n",
    "- to be honest if you don't know, don't try to answer.\n",
    "\n",
    "knowledge : {context}\n",
    "\n",
    "\n",
    "question : {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Bạn là một chatbot chuyên trả lời và đọc hiểu văn bản, hãy sử dụng những thông tin dưới đây để trả lời câu hỏi ở cuối.\n",
    "Luôn trả lời bằng tiếng Việt, không bịa ra câu trả lời, trả lời càng ngắn càng tốt.\n",
    "\n",
    "        Thông tin : {context}\n",
    "\n",
    "\n",
    "        Câu hỏi : {question}\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in medical field. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "Remember:\n",
    "- always answer in Vietnamese.\n",
    "- dont try to generate other answers and questions.\n",
    "- to be honest if you don't know, don't try to answer.\n",
    "\n",
    "knowledge : {context}\n",
    "\n",
    "\n",
    "question : {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "###Câu hỏi :\n",
    "Sử dụng các phần ngữ cảnh sau đây để trả lời câu hỏi ở cuối. Nếu không biết câu trả lời, bạn chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\n",
    "\n",
    "{context}\n",
    "Câu hỏi : {question}\n",
    "###Trả lời :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = \"\"\" USER : You are an artificial intelligence assistant. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "                        Remember:\n",
    "                        - always answer in Vietnamese.\n",
    "                        - don't try to generate other answers and questions.\n",
    "                        - to be honest if you don't know, don't try to answer.\n",
    " \n",
    "                        knowledge : {context}\n",
    " \n",
    " \n",
    "                        question : {question}\n",
    "\n",
    "                        ASSISTANT:\n",
    " \n",
    "                        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bệnh nhân khám ở đâu ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "Đáp án : 0210.627.8888\n",
      "\n",
      "                        Đáp án đúng : 0210.627.8888\n",
      "\n",
      "                        Đáp án sai : 0210.627.8888\n",
      "\n",
      "                        Đáp án khác : 0210.627.8888\n",
      "\n",
      "                        Đáp án chưa rõ : 0210.627.8888\n",
      "\n",
      "                        Đáp án không đúng :\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Số điện thoại của bệnh viện là gì ?',\n",
       " 'result': ' Đáp án : 0210.627.8888\\n \\n                         Đáp án đúng : 0210.627.8888\\n \\n                         Đáp án sai : 0210.627.8888\\n \\n                         Đáp án khác : 0210.627.8888\\n \\n                         Đáp án chưa rõ : 0210.627.8888\\n \\n                         Đáp án không đúng :'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Số điện thoại của bệnh viện là gì ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(\"\\n\")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bệnh nhân bị gì ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
