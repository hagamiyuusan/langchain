{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "hf_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from transformers import  AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer, pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = '/home/huynv43/langchain_rag/dbyte'\n",
    "model_embeddings = \"keepitreal/vietnamese-sbert\"\n",
    "cache_dir = '/home/huynv43/langchain_rag/tmp'\n",
    "# model_name_or_path = \"bkai-foundation-models/vietnamese-llama2-7b-120GB\"\n",
    "model_name_or_path = \"LR-AI-Labs/vbd-llama2-7B-50b-chat\"\n",
    "# model_name_or_path = 'mistralai/Mistral-7B-v0.1'\n",
    "model_kwargs = {'device': 'cuda:0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.08s/it]\n",
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             trust_remote_code=True,\n",
    "                                             token = hf_token,\n",
    "                                             quantization_config=bnb_config\n",
    "                                             )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, model_kwargs=model_kwargs, token = hf_token, padding_side=\"left\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample = True,\n",
    "    top_k = 50,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline = text_pipeline, model_kwargs={\"temperature\": 0.1, \"max_length\":512,'device': 'cuda:0',\"stop\":\"\\n\"})\n",
    "embeddings = HuggingFaceEmbeddings(model_name = model_embeddings,  model_kwargs = model_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(DB_FAISS_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in medical field. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "Remember:\n",
    "- don't add \"Answer\" before answer\n",
    "- always answer in Vietnamese.\n",
    "- don't try to generate other answers and questions.\n",
    "- to be honest if you don't know, don't try to answer.\n",
    "\n",
    "knowledge : {context}\n",
    "\n",
    "\n",
    "question : {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Bạn là một chatbot chuyên trả lời và đọc hiểu văn bản, hãy sử dụng những thông tin dưới đây để trả lời câu hỏi ở cuối.\n",
    "Luôn trả lời bằng tiếng Việt, không bịa ra câu trả lời, trả lời càng ngắn càng tốt.\n",
    "\n",
    "        Thông tin : {context}\n",
    "\n",
    "\n",
    "        Câu hỏi : {question}\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an expert in medical field. Your goals is to provide user useful answer from provided knowledge. Think step by step and never ignore any step.\n",
    "Remember:\n",
    "- always answer in Vietnamese.\n",
    "- dont try to generate other answers and questions.\n",
    "- to be honest if you don't know, don't try to answer.\n",
    "\n",
    "knowledge : {context}\n",
    "\n",
    "\n",
    "question : {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "###Câu hỏi :\n",
    "Sử dụng các phần ngữ cảnh sau đây để trả lời câu hỏi ở cuối. Nếu không biết câu trả lời, bạn chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\n",
    "\n",
    "{context}\n",
    "Câu hỏi : {question}\n",
    "###Trả lời :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Trung tâm y tế huyện\n",
      "2. Phòng khám tư "
     ]
    }
   ],
   "source": [
    "query = \"Bệnh nhân khám ở đâu ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huynv43/miniconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: Xin lỗi bạn ạ, ViVi chưa thấy thông tin số điện thoại nào được đề cập tại đây.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Số điện thoại của bệnh viện là bao nhiêu ?',\n",
       " 'result': 'response: Xin lỗi bạn ạ, ViVi chưa thấy thông tin số điện thoại nào được đề cập tại đây.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Số điện thoại của bệnh viện là bao nhiêu ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(\"\\n\")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bệnh nhân bị gì ?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
